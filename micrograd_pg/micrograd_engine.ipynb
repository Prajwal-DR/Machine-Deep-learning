{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2376d9d",
   "metadata": {},
   "source": [
    "### **Multi layer preceptron** engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa318806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Micrograd import Value\n",
    "import random\n",
    "\n",
    "class neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self,x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "class MLP:\n",
    "\n",
    "    def __init__ (self, nin, nouts):\n",
    "        s = [nin] + nouts\n",
    "        self.layers = [layer(s[i], s[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self,x):\n",
    "        for layers in self.layers:\n",
    "            x = layers(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6c0cee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining input x\n",
    "x = [2.0, 3.0, -1.0]\n",
    "\n",
    "#initializing the MLP with 3 inputs, 4 neurons in the first layer, 4 in the second, and 1 output neuron\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7dffe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.003579289942179254\n",
      "epoch 1, loss: 0.0035966560418362837\n",
      "epoch 2, loss: 0.003614135142820992\n",
      "epoch 3, loss: 0.0036317281483698336\n",
      "epoch 4, loss: 0.0036494359702811626\n",
      "epoch 5, loss: 0.003667259529008045\n",
      "epoch 6, loss: 0.003685199753751889\n",
      "epoch 7, loss: 0.0037032575825573693\n",
      "epoch 8, loss: 0.0037214339624083935\n",
      "epoch 9, loss: 0.0037397298493252774\n",
      "epoch 10, loss: 0.003758146208462926\n",
      "epoch 11, loss: 0.0037766840142103558\n",
      "epoch 12, loss: 0.0037953442502912473\n",
      "epoch 13, loss: 0.0038141279098658247\n",
      "epoch 14, loss: 0.003833035995633804\n",
      "epoch 15, loss: 0.003852069519938719\n",
      "epoch 16, loss: 0.0038712295048732677\n",
      "epoch 17, loss: 0.003890516982386155\n",
      "epoch 18, loss: 0.003909932994390037\n",
      "epoch 19, loss: 0.003929478592870801\n"
     ]
    }
   ],
   "source": [
    "#Training the MLP\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #forward pass\n",
    "    y_pred = [n(x) for x in xs]\n",
    "\n",
    "    #calculating the loss\n",
    "    loss = sum((y_cal - y_out)**2 for y_out, y_cal in zip(y_ex, y_pred))\n",
    "\n",
    "    #print(loss)\n",
    "\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #gradient descent step\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.0001 * p.grad\n",
    "\n",
    "    print(f\"epoch {epoch}, loss: {loss.data}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8a0100b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value (data=0.9709598681090885),\n",
       " Value (data=-0.9891461216809285),\n",
       " Value (data=-0.9824310844255602),\n",
       " Value (data=0.9484279546304176)]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my01env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
